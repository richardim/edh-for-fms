/*************************************************************************************
 High Level Flow
*************************************************************************************/

load
ETL
query
search
sentry
navigator

/*************************************************************************************
 Users
*************************************************************************************/
EDH/HUE/Navigator user:
cloudera

HUE Sentry restricted users:
susie_auditor
frank_inspector

mysql user: cdhsqooper:cdhsqooper

/*************************************************************************************
 Set Up
*************************************************************************************/
Review all scripts for source paths and ip addresses and modify as needed

/*************************************************************************************
 Create users
*************************************************************************************/
default user, drives most of demo, prebuilt with Quickstart VM
  $ cloudera

Add susie_auditor and creates default group with same name
  $ sudo useradd susie_auditor

Add frank_inspector and creates default group with same name
  $ sudo useradd frank_inspector

Create users in HUE via UI admin section.

/*************************************************************************************
 Set up source Database MySQL
*************************************************************************************/
Set up mysql instance with data - sql script contains all required statements to create
users and tables sourced from raw csv files.
  ! You will need to modify your source csv data paths !
  * have a mysql database installed.
  * create database
  * create cdhsqooper user
  * loads csv data
  * ensure driver is already installed on cluster vm
  * need driver for where you install kettle - see kettle install notes
  * also create the final destination export table after transformation job run in edh

/*************************************************************************************
 Pentaho Kettle
*************************************************************************************/
Download and install (extract).  Default folder is data-integration.
  
Configure for cdh50 in plugin.properties
  $KETTLE_HOME/plugins/pentaho-big-data-plugin/plugin.properties
Change active hadoop to
  $KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/cdh50
  
Add your mysql driver jar to
  $KETTLE_HOME/lib/

Configure Job History server - use correct IP of your VM
  $KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/cdh50/mapred-site.xml
      <name>mapreduce.jobhistory.address</name>
      <value>172.16.71.155:10020</value>

Configure YARN RM host - use your correct IP of your VM  
  $KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/cdh50/yarn-site.xml
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>172.16.71.155</value>
      </property>

create job
    * create database connection->New select Hadoop Hive 2 and enter properties:  
      172.16.71.155, finance_data_edh, 10000, uname and pw needed
    * right click on connection in view panel and 'share' the connection
    * create the job in the design panel
    * create database connection->New select MySQL and enter properties:
      localhost, finance_data_edh, 3306, cdhsqooper:cdhsqooper
    * use General->Start as first step in palette
    * use Big Data->Sqoop Import as second step
    * namenode ip 172.16.71.155, 8020
    * jobtracker ip 172.16.71.155, 8032
    * target dir: /user/cloudera
    * use command line view option for sqoop command
    --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh?defaultFetchSize=500&useCursorFetch=true --username cdhsqooper --password cdhsqooper --table tbl_poverty --hive-import --hive-database finance_data_edh -m 1
    --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh?defaultFetchSize=500&useCursorFetch=true --username cdhsqooper --password cdhsqooper --table tbl_poverty --num-mappers 1
    TODO SQOOP import as part of transform job to sqoop in poverty table from Mysql
    * use Scripting->SQL as third step
    * Create outbound link.
    * edit SQL step: give name, use connection you made steps above, enter sql statement to run (c&p from file)
    * click OK and then Save the job.
    
/*************************************************************************************
 Preload database information in HUE
*************************************************************************************/
  * Create database finance_data_edh using "Data Browsers->Metastore Tables"
    * Create 1 table using poverty.csv
  * Create the vw_* before hand.  This may require a non-sentry dry-run to create
  the views.  This allows you to set grants and roles in sentry setup before the execution
  of the real demo, then you can you just drive without having to grant your permissions
  again...
 
/*************************************************************************************
 Sentry Setup
*************************************************************************************/
Setup sentry early. If VM is not set up, refer to sentry set up for hive and impala in CM 5.1 docs. Simply add the service using CM and click through

Hive Service set to use Sentry
  Service-wide->Sentry Service->Sentry
  
Disable impersonation for hiveserver2
  HiveServer2->Enable Impersonation->hive.server2.enable.impersonation, hive.server2.enable.doAs set to false
  
Hive bypass metastore to false
  Service-Wide->Advanced->Bypass Hive Metastore Server->false

Safety valve required in hive
  Service-Wide->Advanced
  <property>
    <name>hive.sentry.testing.mode</name>
    <value>true</value>
  </property>

YARN Service
  NodeManager Base Group->Security->min.user.id = 0

Impala Service set to use Sentry
  Service-wide->Sentry Service->Sentry
  
Restart cluster

Connect using beeline.  Here are the connect strings.  If you deviate from this at all, for instance dont include the driver param, you will log in however, it will not let you run grants/create roles.
  $ beeline -u jdbc:hive2://localhost:10000 -n hive -d org.apache.hive.jdbc.HiveDriver
  or
  $ beeline
  > !connect jdbc:hive2://localhost:10000 hive org.apache.hive.jdbc.HiveDriver

KISS by creating a role that would have all grants to the default cloudera user (this user is default on the quickstart vm)
  > create role db_admin_role;
  > grant all on server server1 to role db_admin_role;
  > grant role db_admin_role to group hive; // maybe not necessary?
  > grant role db_admin_role to group cloudera;

Add cloudera to hive group so he can write to /warehouse directory
  $ sudo usermod -a -G hive cloudera

Create roles and give privileges.  These commands are based on examples required for a demo, you can modify for your environment
  $ beeline -u jdbc:hive2://localhost:10000 -n hive -d org.apache.hive.jdbc.HiveDriver

  > use finance_data_edh;
  
  > create role vw_auditor_role;
  > grant select on table vw_auditor to role vw_auditor_role;
  > grant role vw_auditor_role to group susie_auditor;

  > create role vw_inspector_role;
  > grant select on table vw_inspector to role vw_inspector_role;
  > grant role vw_inspector_role to group frank_inspector;

Modify permissions in hdfs - affects sentry and sqoop job/upload and import data
  $ sudo -u hdfs hdfs dfs -mkdir /user/hive/stage
  $ sudo -u hdfs hdfs dfs -chmod -R 771 /user/hive/stage
  $ sudo -u hdfs hdfs dfs -chown -R hive:hive /user/hive/stage
  $ sudo -u hdfs hdfs dfs -chmod -R 771 /user/hive/warehouse
  $ sudo -u hdfs hdfs dfs -chown -R hive:hive /user/hive/warehouse

/*************************************************************************************
 Search Indexer Scripts
*************************************************************************************/
/* upload csv-indexer files to make the index of the combined table and morphline
  * scp <my path>/csv-indexer.tar cloudera@172.16.71.155:~/
  * tar -xvf ~/csv-indexer.tar
  
/*************************************************************************************
 Data staging
*************************************************************************************/
/* upload data csv (load all tables anyway) into hdfs, assumes files local to cluster 
   box e.g. already scp from host to guest vm */
  * scp <my path>/data.tar cloudera@172.16.71.155:~/datasets/finance_data_edh
  * tar -xvf ~/datasets/finance_data_edh/data.tar
  * $ hadoop fs -ls
    $ hadoop fs -mkdir finance_data_edh
  * Copy csv to hdfs, assumes data is on hdfs box
    $ hadoop fs -copyFromLocal /home/cloudera/datasets/finance_data_edh/*.csv /user/cloudera/finance_data_edh
    $ hadoop fs -ls /user/cloudera/finance_data_edh  

/*************************************************************************************
 Demo
*************************************************************************************/
/* Prelaunch */
Launch VM
Launch MySQLWorkbench to show existing external RDBMS data
Launch Excel with one of the CSV, recommend 8005.csv
Launch Pentaho Spoon

/* Quick introduction to HUE and load first datasets as tables */

  * Show creating hive tables and loading data from hdfs (6005, 8005) via HUE
    * Data Browsers->Metastore Tables pick the finance_data_edh database.
    * Create a new table from a file (already on HDFS) 6005 data.
    * Create a new table from a file, upload to HDFS after viewing in excel locally 8005 data.

3 files are now loaded as tables

/* SQOOP data in from a database - mysql was preferred */
  * sqoop tbl_poverty into edh
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_6005 --hive-import --hive-database finance_data_edh -m 1
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_8005 --hive-import --hive-database finance_data_edh -m 1
  sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_panel --hive-import --hive-database finance_data_edh -m 1
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_poverty --hive-import --hive-database finance_data_edh -m 1
  
4 files are now loaded as tables

/* Run Impala Queries */
select * from tbl_8005;

select a.year, a.taxpayerid, b.name, b.countrycode, b.loginitialgini from tbl_6005 a, tbl_8005 b where a.taxpayerid = b.taxpayerid;

/* ETL Using Kettle */
  * describe kettle palette, various job configurations available, etc.
  * Execute job
  * Explain during execution leverage parallelism of cluster to create new EL Transform

4 tables/files and combined_etl_table should be now fully loaded

/* Run Impala Queries */
select * from combined_etl_table;

select * from combined_etl_table where agencylocationcode > 80000 order by year;

/* Run few queries against combined_etl_table, ensure having run invalidate metadata command in impala before execution */
  * Run select *
  * Run select * where =
  * Run select <certain columns> where =
  * Each one should take time to run and subsequently show the job tracking to explain the relationship to mapreduce
  * Show same queries in Impala.  Select * no where wont have much diff, but others dramatically do

/* Run Impala Queries to highlight RBAC and Sentry
  * Login as susie_auditor
  * Run queries on vw_*
  * Run select * from finance_data_edh.combined_etl_table and highlight error
  * Login as frank_inspector
  * Run queries on vw_*
  * Highlight how each view has different columns, all sourced from combined_etl_table
  * Log into CM and go to Audits
  * susie_auditor illegal access attempt should be logged in Red.
  
/* Export newly transformed combined table to the existing mysql database to show the last
   step of the edh offload */
  * sqoop the data to mysql - fyi - hiveql defaults to delimiting fields by ^A character which is '\001'
  sqoop export --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table combined_export_table --export-dir /user/hive/warehouse/finance_data_edh.db/combined_etl_table -m 1 --input-fields-terminated-by '\001' --input-null-string '\\N' --input-null-non-string '\\N'

/* Index new table - Optional: dry run demo before hand and have dashboard already created
to avoid indexing live after table creation.  Continue to just demoing
  * ./go-solr-csv.sh
  * go to dashboard and create one to demo
  
/* Navigator Demo
  * Show lineage and extraction
  * Apply additional metadata as needed

  
  
  
  
  
  

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
/* Notes */
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- issues with dropping tables or databases using hive metabrowser UI.  You can drop the
full table from the query browser instead: drop database name cascade;

-use finance_data_edh;
-select name,taxpayerid,countrycode,country,year,loginitialgini,growthingini,span,logprivatecredit,inflation,logtrade,logschooling,logcommercialcentralbank,loginitiallowestincomeshare,growthinlowestincomeshare from tbl8005csv;

  * Using raw hive sql will load the data file with header as row, so dont do it unless your
  data is ok to have the header mingled in.
  * do not use 6005 as table name.  use tbl_6005.  Theres a parsing issue with hive query
  where you must use `6005` in your query syntax.
  * Navigator Metadata Server consuming tons of cpu cycle - can stop for prep work since it is just wasting electricity and burning your laptop hot
* beeline connect to do sentry updates
  * !connect jdbc:hive2://localhost:10000 cloudera cloudera org.apache.hive.jdbc.HiveDriver
  * !connect jdbc:hive2://localhost:10000 hive org.apache.hive.jdbc.HiveDriver
  * beeline -u jdbc:hive2://localhost:10000 -n hive -d org.apache.hive.jdbc.HiveDriver
