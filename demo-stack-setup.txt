/* General Flow */
ETL
query
sentry
search
navigator

/* Users */
EDH/HUE users:
cloudera
susie_auditor
frankie_inspector
mysql cdhsqooper:cdhsqooper

/* Set Up */
  * Review all scripts for source paths and ip addresses and modify as needed
  
/* Set up mysql instance with data - script contains all required statements
   You will need to modify your source data paths */
  * have a mysql database installed.
  * create database
  * create cdhsqooper user
  * loads csv data
  * ensure driver is already installed on cluster vm
  * need driver for where you install kettle - see kettle install notes
  * also creates the final destination exported table after transformation job run in edh

/* install pentaho kettle */
  * config for cdh50 in plugin.properties
    * $KETTLE_HOME/plugins/pentaho-big-data-plugin/plugin.properties change active hadoop to $KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/cdh50
    * $KETTLE_HOME/lib/ add your mysql jar
    
/* upload all data csv (load all tables anyway) into hdfs, assumes files local to cluster
   box e.g. already scp from host to guest vm */
  * $ hadoop fs -ls
    $ hadoop fs -mkdir finance_data_edh
  * Copy csv to hdfs, assumes data is on hdfs box
    $ hadoop fs -copyFromLocal /home/cloudera/datasets/finance_data_edh/*.csv /user/cloudera/finance_data_edh
    $ hadoop fs -ls /user/cloudera/finance_data_edh  

/* Create users */
users:
cloudera
susie_auditor
frankie_inspector
    
/* Demo */
/* Quick introduction to HUE and load first datasets as tables */
  * Create database finance_data_edh using "Data Browsers->Metastore Tables"
  * Show creating hive tables and loading data from hdfs (6005, 8005) via HUE
    * Data Browsers->Metastore Tables pick the finance_data_edh database.
    * Create a new table from a file (already on HDFS) 6005 data- do 1.
    * Create a new table from a file, upload to HDFS after viewing in excel locally 8005 data - do 1.

2 files are now loaded as tables

/* SQOOP data in from a database - mysql was preferred */
  * sqoop tbl_panel into edh
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_6005 --hive-import --hive-database finance_data_edh -m 1
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_8005 --hive-import --hive-database finance_data_edh -m 1
  sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_panel --hive-import --hive-database finance_data_edh -m 1
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_poverty --hive-import --hive-database finance_data_edh -m 1
  --target-dir / --connect jdbc:mysql://localhost:3306/finance_data_edh?defaultFetchSize=500&useCursorFetch=true --username cdhsqooper --password cdhsqooper --table tbl_poverty
3 files are now loaded as tables

/* Do some queries in HIVE beeline GUI client - do not show impala yet? */

/* ETL Using Kettle */
  * create job
    * create database connection->New select Hadoop Hive 2 and enter properties:  
      172.16.71.155, finance_data_edh, 10000, uname and pw needed
    * right click on connection in view panel and 'share' the connection
    * create the job in the design panel
    * create database connection->New select MySQL and enter properties:
      localhost, finance_data_edh, 3306, cdhsqooper:cdhsqooper
    * use General->Start as first step in palette
    * use Big Data->Sqoop Import as second step
    * namenode ip 172.16.71.155, 8020
    * jobtracker ip 172.16.71.155, 8032
    * target dir: /user/cloudera
    * use command line view option for sqoop command
    --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh?defaultFetchSize=500&useCursorFetch=true --username cdhsqooper --password cdhsqooper --table tbl_poverty --hive-import --hive-database finance_data_edh -m 1
    --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh?defaultFetchSize=500&useCursorFetch=true --username cdhsqooper --password cdhsqooper --table tbl_poverty --num-mappers 1
    TODO SQOOP import as part of transform job to sqoop in poverty table from Mysql
    * use Scripting->SQL as third step
    * Create outbound link.
    * edit SQL step: give name, use connection you made steps above, enter sql statement to run (c&p from file)
    * click OK and then Save the job.
    * run job

4 tables/files and combined_etl_table should be now fully loaded

/* Run few queries against combined_etl_table, ensure having run invalidate metadata command in impala before execution */
  * Run select *
  * Run select * where =
  * Run select <certain columns> where =
  * Each one should take time to run and subsequently show the job tracking to explain the relationship to mapreduce
  * Show same queries in Impala.  Select * no where wont have much diff, but others dramatically do
  
/* Run HQL scripts to create views */

/* Export newly transformed combined table to the existing mysql database to show the last
   step of the edh offload */
  * sqoop the data to mysql - fyi - hiveql defaults to delimiting fields by ^A character which is '\001'
  sqoop export --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table combined_export_table --export-dir /user/hive/warehouse/finance_data_edh.db/combined_etl_table -m 1 --input-fields-terminated-by '\001' --input-null-string '\\N' --input-null-non-string '\\N'
  
/* run scripts to set up sentry permissions */
setup sentry and permissions on newly created de-normalized table
  - if VM is not set up, refer to sentry set up for hive and impala in CM 5.1 docs
    - leave impersonation for hiveserver2
  - safety valve required in hive
  <property>
<name>sentry.hive.testing.mode</name>
<value>true</value>
</property>
  - NodeManager Base Group / Security for YARN service to 0















*nifty notes*
- issues with dropping tables or databases using hive metabrowser UI.  You can drop the
full table from the query browser instead: drop database name cascade;

-use finance_data_edh;
-select name,taxpayerid,countrycode,country,year,loginitialgini,growthingini,span,logprivatecredit,inflation,logtrade,logschooling,logcommercialcentralbank,loginitiallowestincomeshare,growthinlowestincomeshare from tbl8005csv;

Note
  * Using raw hive sql will load the data file with header as row, so dont do it unless your
  data is ok to have the header mingled in.
  * do not use 6005 as table name.  use tbl_6005.  Theres a parsing issue with hive query
  where you must use `6005` in your query syntax.
  * Navigator Metadata Server consuming tons of cpu cycle - can stop for prep work since it is just wasting electricity and burning your laptop hot