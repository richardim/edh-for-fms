/* General Flow */
ETL
query
sentry
search
navigator

/* Users */
EDH/HUE users:
cloudera
susie_auditor
frank_inspector
mysql cdhsqooper:cdhsqooper

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
/* Set Up */
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Review all scripts for source paths and ip addresses and modify as needed

/* Create users */
default user, drives most of demo, prebuilt with Quickstart VM
cloudera

Add susie_auditor and creates default group with same name
$ sudo useradd susie_auditor

Add frank_inspector and creates default group with same name
$ sudo useradd frank_inspector

Create users in HUE via UI admin section.

/* Set up mysql instance with data - script contains all required statements
   You will need to modify your source data paths */
  * have a mysql database installed.
  * create database
  * create cdhsqooper user
  * loads csv data
  * ensure driver is already installed on cluster vm
  * need driver for where you install kettle - see kettle install notes
  * also creates the final destination exported table after transformation job run in edh

/* install pentaho kettle */
  * config for cdh50 in plugin.properties
    * $KETTLE_HOME/plugins/pentaho-big-data-plugin/plugin.properties change active hadoop to $KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/cdh50
    * $KETTLE_HOME/lib/ add your mysql jar
    * $KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/cdh50/mapred-site.xml
      <name>mapreduce.jobhistory.address</name>
      <value>172.16.71.155:10020</value>
    * $KETTLE_HOME/plugins/pentaho-big-data-plugin/hadoop-configurations/cdh50/yarn-site.xml
      <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>172.16.71.155</value>
      </property>
      
/* Sentry Setup */
Setup sentry early. If VM is not set up, refer to sentry set up for hive and impala in CM 5.1 docs. Simply add the service using CM and click through

Hive Service set to use Sentry
  Service-wide->Sentry Service->Sentry
  
Disable impersonation for hiveserver2
  HiveServer2->Enable Impersonation->hive.server2.enable.impersonation, hive.server2.enable.doAs set to false
  
Hive bypass metastore to false
  Service-Wide->Advanced->Bypass Hive Metastore Server->false

Safety valve required in hive
  Service-Wide->Advanced
  <property>
    <name>hive.sentry.testing.mode</name>
    <value>true</value>
  </property>

YARN Service
  NodeManager Base Group->Security->min.user.id = 0

Impala Service set to use Sentry
  Service-wide->Sentry Service->Sentry
  
Restart cluster

Connect using beeline.  Here are the connect strings.  If you deviate from this at all, for instance dont include the driver param, you will log in however, it will not let you run grants/create roles.
  $ beeline -u jdbc:hive2://localhost:10000 -n hive -d org.apache.hive.jdbc.HiveDriver
  or
  $ beeline
  > !connect jdbc:hive2://localhost:10000 hive org.apache.hive.jdbc.HiveDriver

KISS by creating a role that would have all grants to the default cloudera user (this user is default on the quickstart vm)
  > create role db_admin_role;
  > grant all on server server1 to role db_admin_role;
  > grant role db_admin_role to group hive; // maybe not necessary?
  > grant role db_admin_role to group cloudera;

Add cloudera to hive group so he can write to /warehouse directory
  $ sudo usermod -a -G hive cloudera

Create roles and give privileges.  These commands are based on examples required for a demo, you can modify for your environment
  $ beeline -u jdbc:hive2://localhost:10000 -n hive -d org.apache.hive.jdbc.HiveDriver

  > create role vw_auditor_role;
  > grant select on table vw_auditor to role vw_auditor_role;
  > grant role vw_auditor_role to group susie_auditor;

  > create role vw_inspector_role;
  > grant select on table vw_inspector to role vw_inspector_role;
  > grant role vw_inspector_role to group frank_inspector;

Modify permissions in hdfs
  $ sudo -u hdfs hdfs dfs -mkdir /user/hive/stage
  $ sudo -u hdfs hdfs dfs -chmod -R 771 /user/hive/stage
  $ sudo -u hdfs hdfs dfs -chown -R hive:hive /user/hive/stage
  $ sudo -u hdfs hdfs dfs -chmod -R 771 /user/hive/warehouse
  $ sudo -u hdfs hdfs dfs -chown -R hive:hive /user/hive/warehouse
    
/* upload all data csv (load all tables anyway) into hdfs, assumes files local to cluster 
   box e.g. already scp from host to guest vm */
  * scp <my path>/data.tar cloudera@172.16.71.155:~/datasets/finance_data_edh
  * tar -xvf ~/datasets/finance_data_edh/data.tar
  * $ hadoop fs -ls
    $ hadoop fs -mkdir finance_data_edh
  * Copy csv to hdfs, assumes data is on hdfs box
    $ hadoop fs -copyFromLocal /home/cloudera/datasets/finance_data_edh/*.csv /user/cloudera/finance_data_edh
    $ hadoop fs -ls /user/cloudera/finance_data_edh  

/* upload csv-indexer files to make the index of the combined table and morphline
  * scp <my path>/csv-indexer.tar cloudera@172.16.71.155:~/
  * tar -xvf ~/csv-indexer.tar

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
/* Demo */
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

/* Quick introduction to HUE and load first datasets as tables */
  * Create database finance_data_edh using "Data Browsers->Metastore Tables"
  * Show creating hive tables and loading data from hdfs (6005, 8005) via HUE
    * Data Browsers->Metastore Tables pick the finance_data_edh database.
    * Create a new table from a file (already on HDFS) 6005 data- do 1.
    * Create a new table from a file, upload to HDFS after viewing in excel locally 8005 data - do 1.

2 files are now loaded as tables

/* SQOOP data in from a database - mysql was preferred */
  * sqoop tbl_poverty into edh
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_6005 --hive-import --hive-database finance_data_edh -m 1
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_8005 --hive-import --hive-database finance_data_edh -m 1
  #sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_panel --hive-import --hive-database finance_data_edh -m 1
  sqoop import --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table tbl_poverty --hive-import --hive-database finance_data_edh -m 1
  
3 files are now loaded as tables

/* Do some queries in HIVE beeline GUI client - do not show impala yet? */

/* ETL Using Kettle */
  * create job
    * create database connection->New select Hadoop Hive 2 and enter properties:  
      172.16.71.155, finance_data_edh, 10000, uname and pw needed
    * right click on connection in view panel and 'share' the connection
    * create the job in the design panel
    * create database connection->New select MySQL and enter properties:
      localhost, finance_data_edh, 3306, cdhsqooper:cdhsqooper
    * use General->Start as first step in palette
    * use Big Data->Sqoop Import as second step
    * namenode ip 172.16.71.155, 8020
    * jobtracker ip 172.16.71.155, 8032
    * target dir: /user/cloudera
    * use command line view option for sqoop command
    --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh?defaultFetchSize=500&useCursorFetch=true --username cdhsqooper --password cdhsqooper --table tbl_poverty --hive-import --hive-database finance_data_edh -m 1
    --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh?defaultFetchSize=500&useCursorFetch=true --username cdhsqooper --password cdhsqooper --table tbl_poverty --num-mappers 1
    TODO SQOOP import as part of transform job to sqoop in poverty table from Mysql
    * use Scripting->SQL as third step
    * Create outbound link.
    * edit SQL step: give name, use connection you made steps above, enter sql statement to run (c&p from file)
    * click OK and then Save the job.
    * run job

4 tables/files and combined_etl_table should be now fully loaded

/* Run few queries against combined_etl_table, ensure having run invalidate metadata command in impala before execution */
  * Run select *
  * Run select * where =
  * Run select <certain columns> where =
  * Each one should take time to run and subsequently show the job tracking to explain the relationship to mapreduce
  * Show same queries in Impala.  Select * no where wont have much diff, but others dramatically do
  
/* Run HQL scripts to create views */

/* Export newly transformed combined table to the existing mysql database to show the last
   step of the edh offload */
  * sqoop the data to mysql - fyi - hiveql defaults to delimiting fields by ^A character which is '\001'
  sqoop export --connect jdbc:mysql://172.16.71.1:3306/finance_data_edh --username cdhsqooper --password cdhsqooper --table combined_export_table --export-dir /user/hive/warehouse/finance_data_edh.db/combined_etl_table -m 1 --input-fields-terminated-by '\001' --input-null-string '\\N' --input-null-non-string '\\N'

/* Index new table
  * ./go-solr-csv.sh
  * go to dashboard and create one to demo
  
/* Navigator Demo

  
  
  
  
  
  

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
/* Notes */
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
- issues with dropping tables or databases using hive metabrowser UI.  You can drop the
full table from the query browser instead: drop database name cascade;

-use finance_data_edh;
-select name,taxpayerid,countrycode,country,year,loginitialgini,growthingini,span,logprivatecredit,inflation,logtrade,logschooling,logcommercialcentralbank,loginitiallowestincomeshare,growthinlowestincomeshare from tbl8005csv;

  * Using raw hive sql will load the data file with header as row, so dont do it unless your
  data is ok to have the header mingled in.
  * do not use 6005 as table name.  use tbl_6005.  Theres a parsing issue with hive query
  where you must use `6005` in your query syntax.
  * Navigator Metadata Server consuming tons of cpu cycle - can stop for prep work since it is just wasting electricity and burning your laptop hot
* beeline connect to do sentry updates
  * !connect jdbc:hive2://localhost:10000 cloudera cloudera org.apache.hive.jdbc.HiveDriver
  * !connect jdbc:hive2://localhost:10000 hive org.apache.hive.jdbc.HiveDriver
  * beeline -u jdbc:hive2://localhost:10000 -n hive -d org.apache.hive.jdbc.HiveDriver
